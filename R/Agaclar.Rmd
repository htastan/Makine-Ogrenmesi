---
title: "Ağaç Bazlı Yöntemler"
subtitle: (İktisatçılar İçin) Makine Öğrenmesi (TEK-ES-2021)
author: 
  name: "Prof. Dr. Hüseyin Taştan"
  affiliation: "Yıldız Teknik Üniversitesi"
date: "Haziran 2021"
output:
  html_document: 
    number_sections: true
    theme: lumen
    highlight: haddock 
    # code_folding: show
    toc: yes
    toc_depth: 3
    toc_float: yes
    keep_md: false
---
<style type="text/css"> 
body{
  background-color: #FAFAFA;
  font-size: 18px;
  line-height: 1.8;
}
code.r{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE, results = 'asis', fig.show = 'asis')
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
``` 

<br/>
<br/>


# R ile Sınıflandırma Ağaçları
 
`R`'da ağaç tahmini için çok sayıda paket vardır. Bunlardan biri hem regresyon hem de sınıflama ağaçlarının tahminini yapan `tree` paketidir. Burada örnek olarak  `Carseats` veri setini kullanacağız. `Sales` değişkeni 8 değeri kullanılarak ikili kategorik değişkene dönüştürülmüştür: 
```{r, warning=FALSE}
library(tree)
library(ISLR)
attach(Carseats)
Carseats$High <- factor(ifelse(Carseats$Sales<=8,"No","Yes"))  
```

`High` Satışlar 8'den büyükse "Yes" (1) değilse "No" (0) değerini almaktadır. 

`Sales` değişkenini dışlayarak diğer tüm değişkenlerle `High` için bir ağaç tahmin edelim: 
```{r}
tree.carseats <- tree(High ~ . -Sales, data = Carseats)
summary(tree.carseats) 
```

Eğitim hata oranı %9 olarak bulunmuştur (misclassification error rate).

Tahmin edilen ağacın yaprak sayısı 27'dir (terminal node): 
```{r}
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

Satışların yüksekliğini etkileyen en önemli değişken raf yeridir (`ShelveLoc`, **B**ad, **G**ood, **M**edium). İlk düğümde Kötü ve Orta kalitedeki raf lokasyonları İyi raf lokasyonundan ayrılmaktadır. 

Ağacın tüm detayları: 
```{r}
tree.carseats
```


Test hata oranı: 
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- Carseats$High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset=train)
tree.pred <- predict(tree.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```

```{r}
# correct prediction rate
(104+50)/200
```

Ağacın çapraz geçerleme ile budanması,  `cv.tree()` fonksiyonu: 
```{r} 
set.seed(11)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```

Yukarıdaki çıktıda, `dev` çapraz geçerleme hatasını, `k` ayarlama parametresi $\alpha$'yı ve `size` ağacın büyüklüğünü (terminal düğüm sayısı) göstermektedir. En küçük çapraz geçerleme hatası 74'dür ve yaprak sayısı 8 olan bir ağaca karşılık gelmektedir.  

Ağaç büyüklüğü ve k (cost-complexity parameter)'ye göre hata oranının grafikleri: 
```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```

Şimdi `prune.misclass()` fonksiyonunu kullanarak ağacı budayabiliriz: 
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=8)
plot(prune.carseats)
text(prune.carseats, pretty=0)
```

Not: kitapta 9 yapraklı bir ağaç tahmin edilmiştir: 
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
```

Budanmış ağacın kestirim performansı nasıldır? `predict()` fonksiyonuyla sınıflandırma hatasını tahmin edelim:
```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(97+58)/200
```

<br/>

# R ile Regresyon Ağaçları

Örnek olarak `MASS` paketinde yer alan `Boston` ev fiyatları verisini kullanacağız. Bağımlı değişken medyan ev fiyatları (`medv`). Eğitim verilerinde ağaç tahmini: 
```{r}
library(MASS)
library(tree)
set.seed(81)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

Yukarıdaki çıktıya göre ağaçta 4 değişken year alıyor: "rm"    "lstat" "crim"  "age"  

```{r}
plot(tree.boston)
text(tree.boston, pretty=0)
```

`lstat`: sosyoekonomik statüsü düşük olan hanelerin oranı. Bu değişkenin düşük olduğu yerlerde evler daha pahalıdır. 


Ağacın budanması: 
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
```

Çapraz geçerleme en karmaşık ağacı seçti. Yine de ağacı budamak istersek `prune.tree()` fonksiyonunu kullanabiliriz:
```{r}
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty=0)
```

Budanmamış ağaç üzerinden test hatasının tahmini: 
```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```
Test MSE = 18.93.  


<br/>

# Bagging ve Rassal Ormanlar 

Örnek veri seti = `Boston`. Bagging ve rassal ormanlar için R `randomForest` 
paketi kullanılabilir. Bagging rassal ormanların özel bir halidir (m = p). Bagging tahmini: 
```{r, warning=FALSE}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., 
                           data = Boston, 
                           subset = train, 
                           mtry=13, 
                           importance=TRUE)
bag.boston
```

`mtry=13` ayarlanma parametresi = 13 değişkenin tamamı her ayırımda dikkate alınacak (bagging). Modelin test setindeki performansı: 
```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

Test MSE = 10.13872. 

`randomForest()` fonksiyonunda `ntree` opsiyonu ile ağacın büyüklüğünü değiştirebiliriz:
```{r}
bag.boston <- randomForest(medv ~ ., 
                           data = Boston, 
                           subset = train, 
                           mtry=13, 
                           ntree=25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

Rassal orman tahmini de benzer adımlara sahiptir. Ancak bu durumda daha küçük bir `mtry` değeri kullanırız. Default olarak `randomForest()` fonksiyonu $p/3$ değişkeni kullanır (sınıflama için 
$\sqrt{p}$). Aşağıda `mtry = 6` kullanılmıştır: 
```{r}
# random forest prediction
set.seed(1)
rf.boston <- randomForest(medv ~ ., 
                          data = Boston, 
                          subset = train, 
                          mtry=6, 
                          importance = TRUE)
yhat.rf <- predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

Rassal ormanlar küçük de olsa daha iyi bir test başarısı sergiledi. 

`importance()` fonksiyonunu kullanarak her bir değişkenin önem düzeyini görebiliriz:
```{r}
importance(rf.boston)
```

```{r}
varImpPlot(rf.boston)
```

Sonuçlara göre bölgenin refah düzeyi (`lstat`) ve ev büyüklüğü (`rm`) en önemli değişkenlerdir. 

<br/>

# Boosting 

## `{gbm}` Paketi 

Boosting (takviye) yöntemi için `gbm` paketindeki `gbm()` fonksiyonu kullanılabilir (bkz. [https://www.rdocumentation.org/packages/gbm/versions/2.1.8](https://www.rdocumentation.org/packages/gbm/versions/2.1.8)). 

Örnek olarak `Boston` veri seti için bir takviyeli ağaç tahmin edelim. Bunun için  `gbm()` fonksiyonunu `distribution="gaussian"` opsiyonu ile çalıştıracağız (regresyon problemi olduğu için). İkili sınıflama problemleri için `distribution="bernoulli"` kullanılabilir. `n.trees=5000` opsiyonu 5000 ağaç oluşturulacağını, `interaction.depth=4` her ağacın derinliğini belirtmektedir.  
```{r, warning=FALSE}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., 
                    data = Boston[train,], 
                    distribution = "gaussian",
                    n.trees = 5000, 
                    interaction.depth=4)
```


```{r, warning=FALSE}
summary(boost.boston)
```

Yukarıda `summary()` fonksiyonu her bir değişken için göreceli etki istatistiklerini ve grafiğini vermektedir. Buna göre, önceki sonuçlarla uyumlu olarak, `lstat` ve `rm` en önemli değişkenlerdir. 

Buna ek olarak değişkenlerin kısmi etkilerini de görselleştirebiliriz:
```{r}
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```

Kısmi bağımlılık grafikleri diğer değişkenlerin etkisi arındırıldıktan sonra bir değişkenin çıktı üzerindeki marjinal etkisini göstermektedir. Buna göre medyan ev fiyatları `rm`'ye göre artarken, `lstat` değişkenine göre azalmaktadır. 

Şimdi takviyeli (boosted) ağaç modeline göre `medv` değişkenini test verisinde tahmin edelim
```{r}
yhat.boost <- predict(boost.boston, 
                      newdata=Boston[-train,], 
                      n.trees=5000)
boston.test <- Boston[-train, "medv"]
mean((yhat.boost-boston.test)^2)
```

Yukarıda default $\lambda = 0.001$ (küçültme, shrinkage) parametresi kullanıldı. $\lambda=0.1$ için yeniden tahmin edelim: 
```{r}
boost.boston <- gbm(medv ~ ., 
                    data = Boston[train,],
                    distribution = "gaussian", 
                    n.trees=5000,
                    interaction.depth = 4, 
                    shrinkage = 0.1, 
                    verbose=F)
# kestirimleri test verileriyle hesapla:
yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

Bu durumda MSE daha yüksek çıkmıştır. 

Çapraz geçerleme 
```{r}
# CV
boston.boost.cv <- gbm(medv~., 
                       data = Boston[train,], 
                       distribution = "gaussian", 
                       n.trees=5000, 
                       interaction.depth=4, 
                       shrinkage = 0.1, 
                       verbose=F, 
                       cv.folds=10)

#find the best prediction
best.boost.boston <- gbm.perf(boston.boost.cv)
```

```{r}
yhat.boost = predict(boston.boost.cv, 
                     newdata = Boston[-train,],
                     n.trees = best.boost.boston)
mean((yhat.boost-boston.test)^2)
```

## `{XGBoost}` Paketi 

`{XGBoost}` (eXtreme Gradient Boosting) (bkz. [https://cran.r-project.org/web/packages/xgboost/index.html](https://cran.r-project.org/web/packages/xgboost/index.html) ve [https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)) kütüphanesi gradyan takviye yöntemini uygular. 

```{r}
library(xgboost)
```


```{r}
# eğitim-test ayırımı (yukarıda yapmıştık)
train.boston <- Boston[train,]
test.boston <- Boston[-train,]
```

`{XGBoost}` paketinde verilerin `xgb.DMatrix` nesnesine dönüştürülmesi gerekir (bu sparse ya da dense matrislere izin verir ve böylece hesaplamalar daha etkin yapılabilir). 
```{r}
# veri matrisini oluştur
data_matrix <- as.matrix(train.boston[!names(train.boston) %in% c("medv")])
#
dtrain <- xgb.DMatrix(data = data_matrix, # kestirim değişkenlerini içeren matris
                      label = train.boston$medv # tepki değişkenini içeren vektör 
                      )

boston.xgb <- xgboost(data = dtrain, 
                      max_depth = 2, 
                      eta = 0.2, 
                      nthread = 2, 
                      nrounds = 40, 
                      lambda = 0, 
                      objective = "reg:squarederror")
```


```{r}
# test veri matrisini oluştur
dtest <- as.matrix(test.boston[!names(train.boston) %in% c("medv")])
# kestirimleri hesapla
yhat.xgb <- predict(boston.xgb, dtest)
# MSE hesapla
mean((yhat.xgb - test.boston$medv)^2)
```

```{r}
set.seed(123456)
param <- list("max_depth" = 3, 
              "eta" = 0.2, 
              "objective" = "reg:squarederror", 
              "lambda" = 0)
cv.nround <- 500
cv.nfold <- 5
boston.xgb.cv <- xgb.cv(param = param, 
                        data = dtrain, 
                        nfold = cv.nfold, 
                        nrounds = cv.nround,
                        early_stopping_rounds = 200, # 
                        verbose=0)
```

```{r}
boston.xgb = xgboost(param=param, 
                     data=dtrain, 
                     nthread=2, 
                     nrounds=boston.xgb.cv$best_iteration, 
                     verbose=0)
```

```{r}
yhat.xgb <- predict(boston.xgb,dtest)
mean((yhat.xgb - test.boston$medv)^2)
```
```{r}
importance <- xgb.importance(colnames(train.boston[!names(train.boston) %in% c("medv")]), model=boston.xgb)
importance
```

```{r}
xgb.plot.importance(importance, rel_to_first=TRUE, xlab="Relative Importance")
```

## Tidymodels 

```{r}
library(tidyverse)
library(tidymodels)
library(xgboost)
```

```{r}
# determine training and test sets
# 75% train, 25% test, with stratified sampling based on medv

set.seed(135)
boston_split <- initial_split(Boston, prop=0.75, strata=medv)

boston_train <- boston_split %>% training()

boston_test <- boston_split %>% testing()
```

```{r}
xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune(),                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgb_spec
```


```{r}
# hyperparameter grid
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), boston_train),
  learn_rate(),
  size = 30
)
xgb_grid
```


```{r}
xgb_wf <- workflow() %>%
  add_formula(medv ~ .) %>%
  add_model(xgb_spec)

xgb_wf
```


```{r}
# CV folds 
set.seed(123)
boston_folds <- vfold_cv(boston_train, v = 5, strata = medv)

boston_folds
```


```{r}
library(doParallel)
doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = boston_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res
```


```{r}
collect_metrics(xgb_res)
```

```{r}
xgb_res %>% collect_metrics(summarize=FALSE)
```


```{r}
xgb_res %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == 'rmse') %>%
  group_by(id) %>%
  summarize(min_rmse = min(.estimate),
            median_rmse = median(.estimate),
            max_rmse = max(.estimate))
```


```{r}
xgb_res %>%
  show_best(metric = 'rmse', n = 5)
```


```{r}
xgb_res %>%
  show_best(metric = 'rsq', n = 5)
```

```{r}
best_xgb_model <- xgb_res %>%
  select_best(metric = 'rmse')
best_xgb_model
```


```{r}
# finalize the workflow (fixes the hyperparameters)
final_boston_wkfl <- xgb_wf %>%
  finalize_workflow(best_xgb_model)
final_boston_wkfl
```


```{r}
# train the decision tree 
boston_final_fit <- final_boston_wkfl %>%
  last_fit(split = boston_split)
boston_final_fit %>%
  collect_metrics()

```
```{r}
library(vip)

boston_final_fit %>%
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) 
```



<br/>

# Örnek: Titanic 

Bu örnekte Titanic kazasında hayatını kaybedenler için bir sınıflandırma ağacı oluşturmaya çalışacağız (detaylar için bkz. [RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) ). Bu trajik kaza çok fazla kişinin ölümü ile sonuçlanmıştır. Böyle kazaların tekrar yaşanmaması için çok sayıda güvenlik tedbirlerinin alınmasında ve yasal düzenleme yapılmasında etkili olmuştur. 

Sorular: acaba bu kadar kişinin ölümünün ardındaki nedenler nelerdir? Hangi değişkenler bireyin hayatta kalma olasılığı üzerinde önemli bir etkiye sahiptir?

Önce verileri `R`'a tanıtalım: 
```{r, warning=FALSE}
library(tidyverse)
library(modelr)
library(broom)
set.seed(1234)

theme_set(theme_minimal())

library(titanic)
# use the training data 
titanic <- titanic_train %>%
  as_tibble()

titanic %>%
  head() %>%
  knitr::kable()
```


Çıktı değişkenini oluştur: 
```{r}
titanic_tree_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived", "Died"),
         Survived = as.factor(Survived),
         Sex = as.factor(Sex))
titanic_tree_data
table(titanic_tree_data$Survived)
```

Sadece  `age` ve `sex` özniteliklerini kullanarak küçük bir ağaç tahmin edelim. Bunun için `partykit` paketini kullanacağız: 
```{r, warning=FALSE}
library(partykit)
titanic_tree <- ctree(Survived ~ Age + Sex, data = titanic_tree_data)
titanic_tree
```

Oluşturduğumuz ağaç 3 son düğüme (yaprak) ve 2 iç düğüme sahiptir. Ağacın görselleştirilmesi: 
```{r}
plot(titanic_tree)
```

Sınıflandırma hatası: 
```{r}
# compute predicted values
pred1 <- predict(titanic_tree) 
# true classifications
predsuccess1 <- (pred1 == titanic_tree_data$Survived)
# wrong classifications
prederror1 <- !predsuccess1
# average misclassification rate: 
mean(prederror1, na.rm = TRUE)

# or just in one line: 
# 1 - mean(predict(titanic_tree) == titanic_tree_data$Survived, na.rm = TRUE)
```

Bu basit ağaçla bile hata oranı yaklaşık % 21 olarak bulundu. 

Veri setinde yer alan diğer öznitelikleri de kullanarak daha karmaşık bir ağaç tahmin edelim. Önce karakter değişkenleri faktör değişkenine dönüştürelim: 
```{r}
titanic_tree_full_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived",
                            if_else(Survived == 0, "Died", NA_character_))) %>%
  mutate_if(is.character, as.factor)
```

Ağacı tahmin et:  
```{r}

titanic_tree_full <- ctree(Survived ~ Pclass + Sex + Age + SibSp +
                             Parch + Fare + Embarked,
                           data = titanic_tree_full_data)
titanic_tree_full

```

```{r}
plot(titanic_tree_full,  ip_args = list(pval = FALSE, id = FALSE),
     tp_args = list( id = FALSE) )
```

Hata oranı: 
```{r}
# error rate
1 - mean(predict(titanic_tree_full) == titanic_tree_data$Survived,
         na.rm = TRUE)
```

Hata oranı daha da düştü, %18.3. 


Şimdi `caret` paketini kullanarak bir rassal orman tahmin edelim. Bunun için  `caret::train()` fonksiyonunu kullanacağız:  
```{r}
# drop the NA obs.
titanic_rf_data <- titanic_tree_full_data %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  drop_na()
titanic_rf_data
```

```{r, warning=FALSE}
# build a random forest with ntree=200
library(caret)
titanic_rf <- train(Survived ~ ., data = titanic_rf_data,
                    method = "rf",
                    ntree = 200,
                    trControl = trainControl(method = "oob"))
titanic_rf
```



```{r}
str(titanic_rf, max.level = 1)
# Info on the final model: 
titanic_rf$finalModel
```

```{r}
# confusion table 
knitr::kable(titanic_rf$finalModel$confusion)
```



```{r}
# look at an individual tree
randomForest::getTree(titanic_rf$finalModel, labelVar = TRUE)
```



```{r}
# var importance plot 
randomForest::varImpPlot(titanic_rf$finalModel)
```



<br>
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

